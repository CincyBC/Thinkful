{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import ensemble\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.425966</td>\n",
       "      <td>0.960523</td>\n",
       "      <td>1.141109</td>\n",
       "      <td>-0.168252</td>\n",
       "      <td>0.420987</td>\n",
       "      <td>-0.029728</td>\n",
       "      <td>0.476201</td>\n",
       "      <td>0.260314</td>\n",
       "      <td>-0.568671</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208254</td>\n",
       "      <td>-0.559825</td>\n",
       "      <td>-0.026398</td>\n",
       "      <td>-0.371427</td>\n",
       "      <td>-0.232794</td>\n",
       "      <td>0.105915</td>\n",
       "      <td>0.253844</td>\n",
       "      <td>0.081080</td>\n",
       "      <td>3.67</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.229658</td>\n",
       "      <td>0.141004</td>\n",
       "      <td>0.045371</td>\n",
       "      <td>1.202613</td>\n",
       "      <td>0.191881</td>\n",
       "      <td>0.272708</td>\n",
       "      <td>-0.005159</td>\n",
       "      <td>0.081213</td>\n",
       "      <td>0.464960</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.167716</td>\n",
       "      <td>-0.270710</td>\n",
       "      <td>-0.154104</td>\n",
       "      <td>-0.780055</td>\n",
       "      <td>0.750137</td>\n",
       "      <td>-0.257237</td>\n",
       "      <td>0.034507</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>4.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.0</td>\n",
       "      <td>-0.644269</td>\n",
       "      <td>1.417964</td>\n",
       "      <td>1.074380</td>\n",
       "      <td>-0.492199</td>\n",
       "      <td>0.948934</td>\n",
       "      <td>0.428118</td>\n",
       "      <td>1.120631</td>\n",
       "      <td>-3.807864</td>\n",
       "      <td>0.615375</td>\n",
       "      <td>...</td>\n",
       "      <td>1.943465</td>\n",
       "      <td>-1.015455</td>\n",
       "      <td>0.057504</td>\n",
       "      <td>-0.649709</td>\n",
       "      <td>-0.415267</td>\n",
       "      <td>-0.051634</td>\n",
       "      <td>-1.206921</td>\n",
       "      <td>-1.085339</td>\n",
       "      <td>40.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7.0</td>\n",
       "      <td>-0.894286</td>\n",
       "      <td>0.286157</td>\n",
       "      <td>-0.113192</td>\n",
       "      <td>-0.271526</td>\n",
       "      <td>2.669599</td>\n",
       "      <td>3.721818</td>\n",
       "      <td>0.370145</td>\n",
       "      <td>0.851084</td>\n",
       "      <td>-0.392048</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.073425</td>\n",
       "      <td>-0.268092</td>\n",
       "      <td>-0.204233</td>\n",
       "      <td>1.011592</td>\n",
       "      <td>0.373205</td>\n",
       "      <td>-0.384157</td>\n",
       "      <td>0.011747</td>\n",
       "      <td>0.142404</td>\n",
       "      <td>93.20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9.0</td>\n",
       "      <td>-0.338262</td>\n",
       "      <td>1.119593</td>\n",
       "      <td>1.044367</td>\n",
       "      <td>-0.222187</td>\n",
       "      <td>0.499361</td>\n",
       "      <td>-0.246761</td>\n",
       "      <td>0.651583</td>\n",
       "      <td>0.069539</td>\n",
       "      <td>-0.736727</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.246914</td>\n",
       "      <td>-0.633753</td>\n",
       "      <td>-0.120794</td>\n",
       "      <td>-0.385050</td>\n",
       "      <td>-0.069733</td>\n",
       "      <td>0.094199</td>\n",
       "      <td>0.246219</td>\n",
       "      <td>0.083076</td>\n",
       "      <td>3.68</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "5   2.0 -0.425966  0.960523  1.141109 -0.168252  0.420987 -0.029728  0.476201   \n",
       "6   4.0  1.229658  0.141004  0.045371  1.202613  0.191881  0.272708 -0.005159   \n",
       "7   7.0 -0.644269  1.417964  1.074380 -0.492199  0.948934  0.428118  1.120631   \n",
       "8   7.0 -0.894286  0.286157 -0.113192 -0.271526  2.669599  3.721818  0.370145   \n",
       "9   9.0 -0.338262  1.119593  1.044367 -0.222187  0.499361 -0.246761  0.651583   \n",
       "\n",
       "         V8        V9  ...         V21       V22       V23       V24  \\\n",
       "0  0.098698  0.363787  ...   -0.018307  0.277838 -0.110474  0.066928   \n",
       "1  0.085102 -0.255425  ...   -0.225775 -0.638672  0.101288 -0.339846   \n",
       "2  0.247676 -1.514654  ...    0.247998  0.771679  0.909412 -0.689281   \n",
       "3  0.377436 -1.387024  ...   -0.108300  0.005274 -0.190321 -1.175575   \n",
       "4 -0.270533  0.817739  ...   -0.009431  0.798278 -0.137458  0.141267   \n",
       "5  0.260314 -0.568671  ...   -0.208254 -0.559825 -0.026398 -0.371427   \n",
       "6  0.081213  0.464960  ...   -0.167716 -0.270710 -0.154104 -0.780055   \n",
       "7 -3.807864  0.615375  ...    1.943465 -1.015455  0.057504 -0.649709   \n",
       "8  0.851084 -0.392048  ...   -0.073425 -0.268092 -0.204233  1.011592   \n",
       "9  0.069539 -0.736727  ...   -0.246914 -0.633753 -0.120794 -0.385050   \n",
       "\n",
       "        V25       V26       V27       V28  Amount  Class  \n",
       "0  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.167170  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3  0.647376 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4 -0.206010  0.502292  0.219422  0.215153   69.99      0  \n",
       "5 -0.232794  0.105915  0.253844  0.081080    3.67      0  \n",
       "6  0.750137 -0.257237  0.034507  0.005168    4.99      0  \n",
       "7 -0.415267 -0.051634 -1.206921 -1.085339   40.80      0  \n",
       "8  0.373205 -0.384157  0.011747  0.142404   93.20      0  \n",
       "9 -0.069733  0.094199  0.246219  0.083076    3.68      0  \n",
       "\n",
       "[10 rows x 31 columns]"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_raw = pd.read_csv('creditcard.csv')\n",
    "sms_raw.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: int64)"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count nulls \n",
    "null_count = sms_raw.isnull().sum()\n",
    "null_count[null_count>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    284315\n",
      "1       492\n",
      "Name: Class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(sms_raw['Class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    868\n",
      "1    492\n",
      "Name: Class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Try to bring them more in balance\n",
    "fraud = sms_raw.loc[np.where(sms_raw['Class'] == 1)]\n",
    "non_fraud = sms_raw.loc[np.where(sms_raw['Class'] == 0)]\n",
    "\n",
    "# Take a random sample from the no's\n",
    "msk = np.random.rand(len(non_fraud)) < 0.003\n",
    "no_data = non_fraud[msk]\n",
    "\n",
    "# Join the no data back with the yes's\n",
    "df2 = no_data.append(fraud).dropna()\n",
    "\n",
    "print(df2['Class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x14fd86eb8>"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAERCAYAAAB1k2wJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQO84jtPL8YHecRynl+MDveM4Ti/n/wPWLjIPvbWCPgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x14d2ad780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#See how the features relate to each other through a heatmap\n",
    "sns.heatmap(df2.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up our training/testing data\n",
    "msk = np.random.rand(len(df2)) < 0.8\n",
    "train_data = df2[msk]\n",
    "test_data = df2[~msk]\n",
    "\n",
    "X_train = train_data.drop(['Class'], 1)\n",
    "y_train = train_data['Class']\n",
    "X_test = test_data.drop(['Class'], 1)\n",
    "y_test = test_data['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Try by changing the features through PCA.\n",
    "pca = PCA(n_components=5)\n",
    "X_std_pca = pca.fit_transform(X_train)\n",
    "X1 = pd.DataFrame(X_std_pca)\n",
    "\n",
    "pca = PCA(n_components=5)\n",
    "X_std_pca = pca.fit_transform(X_test)\n",
    "X2 = pd.DataFrame(X_std_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first try Naive Bayes since it is famous for being used to detect credit card fraud.\n",
    "\n",
    "For each of the models, we'll get the overall accuracy score and then break down their score for type I and type II errors. Since we are focusing on catching fraudulent charges, it's helpful to break it down to false positives and false negatives. We then take their cross validation score to make sure it's not overly fitted to a pocket of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score of this model is: 0.9423076923076923\n",
      "Training set accuracy:\n",
      "Percent Type I errors: 0.010073260073260074\n",
      "Percent Type II errors: 0.047619047619047616\n",
      "\n",
      "Test set accuracy:\n",
      "Percent Type I errors: 0.026119402985074626\n",
      "Percent Type II errors: 0.05223880597014925\n",
      "Run this model through cross validation: \n",
      "[ 0.96363636  0.97272727  0.98181818  0.88181818  0.91743119  0.9266055\n",
      "  0.98165138  0.91743119  0.94444444  0.93518519]\n"
     ]
    }
   ],
   "source": [
    "#Let's put our variables through Naive Bayes.\n",
    "bnb = BernoulliNB()\n",
    "\n",
    "#Fit the model.\n",
    "bnb.fit(X1, y_train)\n",
    "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)\n",
    "\n",
    "#Take the score.\n",
    "print('The accuracy score of this model is: {}'.format(bnb.score(X1, y_train)))\n",
    "predict_train = bnb.predict(X1)\n",
    "predict_test = bnb.predict(X2)\n",
    "\n",
    "# Accuracy tables.\n",
    "table_train = pd.crosstab(y_train, predict_train, margins=True)\n",
    "table_test = pd.crosstab(y_test, predict_test, margins=True)\n",
    "\n",
    "train_tI_errors = table_train.loc[0.0,1.0] / table_train.loc['All','All']\n",
    "train_tII_errors = table_train.loc[1.0,0.0] / table_train.loc['All','All']\n",
    "\n",
    "test_tI_errors = table_test.loc[0.0,1.0]/table_test.loc['All','All']\n",
    "test_tII_errors = table_test.loc[1.0,0.0]/table_test.loc['All','All']\n",
    "\n",
    "print((\n",
    "    'Training set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}\\n\\n'\n",
    "    'Test set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}'\n",
    ").format(train_tI_errors, train_tII_errors, test_tI_errors, test_tII_errors))\n",
    "print('Run this model through cross validation: \\n{}'.format(cross_val_score(bnb, X1, y_train, cv=10)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bernoulli Bayes did a good job at predicting the fraud cases with relative few false positives and few false negatives.\n",
    "\n",
    "Let's give Logistic regression (Ridge/Lasso), KNN, SVC and Random Forest a try to see how they compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score for this model is: 0.9496336996336996\n",
      "Training set accuracy:\n",
      "Percent Type I errors: 0.009157509157509158\n",
      "Percent Type II errors: 0.04120879120879121\n",
      "\n",
      "Test set accuracy:\n",
      "Percent Type I errors: 0.014925373134328358\n",
      "Percent Type II errors: 0.03731343283582089\n",
      "Run this model through cross validation: \n",
      "[ 0.99090909  0.83636364  0.97272727  0.9         0.89908257  0.9266055\n",
      "  0.98165138  0.93577982  0.9537037   0.92592593]\n"
     ]
    }
   ],
   "source": [
    "#Let's put our variables through Lasso.\n",
    "lr = LogisticRegression(C=1, penalty='l1')\n",
    "\n",
    "#Fit the model.\n",
    "lr.fit(X1, y_train)\n",
    "\n",
    "print('The accuracy score for this model is: {}'.format(lr.score(X1, y_train)))\n",
    "predict_train = lr.predict(X1)\n",
    "predict_test = lr.predict(X2)\n",
    "\n",
    "# Accuracy tables.\n",
    "table_train = pd.crosstab(y_train, predict_train, margins=True)\n",
    "table_test = pd.crosstab(y_test, predict_test, margins=True)\n",
    "\n",
    "train_tI_errors = table_train.loc[0.0,1.0] / table_train.loc['All','All']\n",
    "train_tII_errors = table_train.loc[1.0,0.0] / table_train.loc['All','All']\n",
    "\n",
    "test_tI_errors = table_test.loc[0.0,1.0]/table_test.loc['All','All']\n",
    "test_tII_errors = table_test.loc[1.0,0.0]/table_test.loc['All','All']\n",
    "\n",
    "print((\n",
    "    'Training set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}\\n\\n'\n",
    "    'Test set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}'\n",
    ").format(train_tI_errors, train_tII_errors, test_tI_errors, test_tII_errors))\n",
    "print('Run this model through cross validation: \\n{}'.format(cross_val_score(lr, X1, y_train, cv=10)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score for this model is: 0.9294871794871795\n",
      "Training set accuracy:\n",
      "Percent Type I errors: 0.0009157509157509158\n",
      "Percent Type II errors: 0.0695970695970696\n",
      "\n",
      "Test set accuracy:\n",
      "Percent Type I errors: 0.0\n",
      "Percent Type II errors: 0.055970149253731345\n",
      "Run this model through cross validation: \n",
      "[ 0.96363636  0.95454545  0.97272727  0.90909091  0.88990826  0.86238532\n",
      "  0.99082569  0.90825688  0.91666667  0.91666667]\n"
     ]
    }
   ],
   "source": [
    "#Let's put our variables through Ridge Regression.\n",
    "\n",
    "lr = LogisticRegression(C=1, penalty='l2')\n",
    "\n",
    "#Fit the model.\n",
    "lr.fit(X1, y_train)\n",
    "\n",
    "print('The accuracy score for this model is: {}'.format(lr.score(X1, y_train)))\n",
    "\n",
    "predict_train = lr.predict(X1)\n",
    "predict_test = lr.predict(X2)\n",
    "\n",
    "# Accuracy tables.\n",
    "table_train = pd.crosstab(y_train, predict_train, margins=True)\n",
    "table_test = pd.crosstab(y_test, predict_test, margins=True)\n",
    "\n",
    "train_tI_errors = table_train.loc[0.0,1.0] / table_train.loc['All','All']\n",
    "train_tII_errors = table_train.loc[1.0,0.0] / table_train.loc['All','All']\n",
    "\n",
    "test_tI_errors = table_test.loc[0.0,1.0]/table_test.loc['All','All']\n",
    "test_tII_errors = table_test.loc[1.0,0.0]/table_test.loc['All','All']\n",
    "\n",
    "print((\n",
    "    'Training set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}\\n\\n'\n",
    "    'Test set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}'\n",
    ").format(train_tI_errors, train_tII_errors, test_tI_errors, test_tII_errors))\n",
    "print('Run this model through cross validation: \\n{}'.format(cross_val_score(lr, X1, y_train, cv=10)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score for this model is: 0.7884615384615384\n",
      "Training set accuracy:\n",
      "Percent Type I errors: 0.07142857142857142\n",
      "Percent Type II errors: 0.1401098901098901\n",
      "\n",
      "Test set accuracy:\n",
      "Percent Type I errors: 0.10820895522388059\n",
      "Percent Type II errors: 0.29850746268656714\n",
      "Run this model through cross validation: \n",
      "[ 0.35454545  0.14545455  0.03636364  0.01818182  0.01834862  0.39449541\n",
      "  0.23853211  0.28440367  0.36111111  0.66666667]\n"
     ]
    }
   ],
   "source": [
    "#Let's put our variables through a KNN Classifier.\n",
    "\n",
    "neighbors = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "#Fit the model.\n",
    "neighbors.fit(X1, y_train)\n",
    "\n",
    "\n",
    "print('The accuracy score for this model is: {}'.format(neighbors.score(X1, y_train)))\n",
    "predict_train = neighbors.predict(X1)\n",
    "predict_test = neighbors.predict(X2)\n",
    "\n",
    "# Accuracy tables.\n",
    "table_train = pd.crosstab(y_train, predict_train, margins=True)\n",
    "table_test = pd.crosstab(y_test, predict_test, margins=True)\n",
    "\n",
    "train_tI_errors = table_train.loc[0.0,1.0] / table_train.loc['All','All']\n",
    "train_tII_errors = table_train.loc[1.0,0.0] / table_train.loc['All','All']\n",
    "\n",
    "test_tI_errors = table_test.loc[0.0,1.0]/table_test.loc['All','All']\n",
    "test_tII_errors = table_test.loc[1.0,0.0]/table_test.loc['All','All']\n",
    "\n",
    "print((\n",
    "    'Training set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}\\n\\n'\n",
    "    'Test set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}'\n",
    ").format(train_tI_errors, train_tII_errors, test_tI_errors, test_tII_errors))\n",
    "print('Run this model through cross validation: \\n{}'.format(cross_val_score(neighbors, X1, y_train, cv=10)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score for this model is: 0.9917582417582418\n",
      "Training set accuracy:\n",
      "Percent Type I errors: 0.0018315018315018315\n",
      "Percent Type II errors: 0.00641025641025641\n",
      "\n",
      "Test set accuracy:\n",
      "Percent Type I errors: 0.22761194029850745\n",
      "Percent Type II errors: 0.029850746268656716\n",
      "Run this model through cross validation: \n",
      "[ 0.44545455  0.97272727  0.95454545  0.86363636  0.89908257  0.9266055\n",
      "  0.98165138  0.89908257  0.94444444  0.89814815]\n"
     ]
    }
   ],
   "source": [
    "#Let's put our variables through a random forest classifer.\n",
    "\n",
    "rfc = ensemble.RandomForestClassifier(n_estimators=10, criterion='gini', max_depth=None, min_samples_split=2, \n",
    "                                      min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', \n",
    "                                      max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, \n",
    "                                      bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0, \n",
    "                                      warm_start=False, class_weight=None)\n",
    "#Fit the model\n",
    "rfc.fit(X1, y_train)\n",
    "\n",
    "predict_train = rfc.predict(X1)\n",
    "predict_test = rfc.predict(X2)\n",
    "\n",
    "# Accuracy tables.\n",
    "table_train = pd.crosstab(y_train, predict_train, margins=True)\n",
    "table_test = pd.crosstab(y_test, predict_test, margins=True)\n",
    "\n",
    "train_tI_errors = table_train.loc[0.0,1.0] / table_train.loc['All','All']\n",
    "train_tII_errors = table_train.loc[1.0,0.0] / table_train.loc['All','All']\n",
    "\n",
    "test_tI_errors = table_test.loc[0.0,1.0]/table_test.loc['All','All']\n",
    "test_tII_errors = table_test.loc[1.0,0.0]/table_test.loc['All','All']\n",
    "print('The accuracy score for this model is: {}'.format(rfc.score(X1, y_train)))\n",
    "print((\n",
    "    'Training set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}\\n\\n'\n",
    "    'Test set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}'\n",
    ").format(train_tI_errors, train_tII_errors, test_tI_errors, test_tII_errors))\n",
    "print('Run this model through cross validation: \\n{}'.format(cross_val_score(rfc, X1, y_train, cv=10)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score for this model is: 0.9487179487179487\n",
      "Training set accuracy:\n",
      "Percent Type I errors: 0.013736263736263736\n",
      "Percent Type II errors: 0.037545787545787544\n",
      "\n",
      "Test set accuracy:\n",
      "Percent Type I errors: 0.048507462686567165\n",
      "Percent Type II errors: 0.029850746268656716\n",
      "Run this model through cross validation: \n",
      "[ 0.81818182  0.95454545  0.95454545  0.86363636  0.91743119  0.9266055\n",
      "  0.98165138  0.88990826  0.93518519  0.89814815]\n"
     ]
    }
   ],
   "source": [
    "#Let's put our variables through a gradient booster.\n",
    "\n",
    "clf = ensemble.GradientBoostingClassifier(loss='exponential', learning_rate=1.0, n_estimators=10, subsample=1.0, \n",
    "                                          criterion='friedman_mse', min_samples_split=1.0, min_samples_leaf=1, \n",
    "                                          min_weight_fraction_leaf=0.0, max_depth=None, min_impurity_decrease=0.0, \n",
    "                                          min_impurity_split=None, init=None, random_state=None, max_features=None, \n",
    "                                          verbose=0, max_leaf_nodes=None, warm_start=False, presort='auto')\n",
    "#Fit the model.\n",
    "clf.fit(X1, y_train)\n",
    "\n",
    "predict_train = clf.predict(X1)\n",
    "predict_test = clf.predict(X2)\n",
    "\n",
    "# Accuracy tables.\n",
    "table_train = pd.crosstab(y_train, predict_train, margins=True)\n",
    "table_test = pd.crosstab(y_test, predict_test, margins=True)\n",
    "\n",
    "train_tI_errors = table_train.loc[0.0,1.0] / table_train.loc['All','All']\n",
    "train_tII_errors = table_train.loc[1.0,0.0] / table_train.loc['All','All']\n",
    "\n",
    "test_tI_errors = table_test.loc[0.0,1.0]/table_test.loc['All','All']\n",
    "test_tII_errors = table_test.loc[1.0,0.0]/table_test.loc['All','All']\n",
    "print('The accuracy score for this model is: {}'.format(clf.score(X1, y_train)))\n",
    "print((\n",
    "    'Training set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}\\n\\n'\n",
    "    'Test set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}'\n",
    ").format(train_tI_errors, train_tII_errors, test_tI_errors, test_tII_errors))\n",
    "print('Run this model through cross validation: \\n{}'.format(cross_val_score(clf, X1, y_train, cv=10)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bernoulli and Ridge models seemed to work the best when taken in a complete picture. Gradient Boosting performed well with a 0.957 accuracy and relatively low type I and type II error rates, but the cross validation scores underperformed. This was the same problem that afflicted Random Forest. Ideally, the accuracy score should be closer to how KNN performed, but KNN had abysmal cross validation scores.\n",
    "\n",
    "The models that performed the best out of all of the models was Bernoulli Bayes and Ridge Logistic Regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
