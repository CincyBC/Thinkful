{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utility function for standard text cleaning.\n",
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'.  Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "    \n",
    "# Load and clean the data.\n",
    "persuasion = gutenberg.raw('austen-persuasion.txt')\n",
    "alice = gutenberg.raw('carroll-alice.txt')\n",
    "emma = gutenberg.raw('austen-emma.txt')\n",
    "whitman = gutenberg.raw('whitman-leaves.txt')\n",
    "\n",
    "# The Chapter indicator is idiosyncratic\n",
    "persuasion = re.sub(r'Chapter \\d+', '', persuasion)\n",
    "alice = re.sub(r'CHAPTER .*', '', alice)\n",
    "emma = re.sub(r'VOLUME \\w+', '', emma)\n",
    "emma = re.sub(r'CHAPTER \\w+', '', emma)\n",
    "whitman = re.sub(r'VOLUME \\w+', '', whitman)\n",
    "whitman = re.sub(r'CHAPTER \\w+', '', whitman)\n",
    "    \n",
    "alice = text_cleaner(alice)\n",
    "persuasion = text_cleaner(persuasion)\n",
    "emma = text_cleaner(emma)\n",
    "whitman = text_cleaner(whitman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice Parsed 0:00:13.896927\n",
      "Persuasion Parsed 0:00:38.817070\n",
      "Emma Parsed 0:01:34.714986\n",
      "Whitman Parsed 0:02:09.210663\n",
      "\n",
      "It took the following time to complete this task: 0:02:09.211653\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "# Parse the cleaned novels. This can take a bit.\n",
    "nlp = spacy.load('en')\n",
    "alice_doc = nlp(alice)\n",
    "print('Alice Parsed', datetime.now() - start)\n",
    "persuasion_doc = nlp(persuasion)\n",
    "print('Persuasion Parsed', datetime.now() - start)\n",
    "emma_doc = nlp(emma)\n",
    "print('Emma Parsed', datetime.now() - start)\n",
    "whitman_doc = nlp(whitman)\n",
    "print('Whitman Parsed', datetime.now() - start)\n",
    "print('\\nIt took the following time to complete this task:', datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Oh, dear, !)</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(I, shall, be, late, !, ')</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0        1\n",
       "0  (Alice, was, beginning, to, get, very, tired, ...  Carroll\n",
       "1  (So, she, was, considering, in, her, own, mind...  Carroll\n",
       "2  (There, was, nothing, so, VERY, remarkable, in...  Carroll\n",
       "3                                      (Oh, dear, !)  Carroll\n",
       "4                         (I, shall, be, late, !, ')  Carroll"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group into sentences.\n",
    "alice_sents = [[sent, \"Carroll\"] for sent in alice_doc.sents]\n",
    "persuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents]\n",
    "emma_sents = [[sent, \"Austen\"] for sent in emma_doc.sents]\n",
    "emma_sents = emma_sents[0:len(alice_sents)]\n",
    "whitman_sents = [[sent, \"Whitman\"] for sent in whitman_doc.sents]\n",
    "whitman_sents = whitman_sents[0:len(alice_sents)]\n",
    "\n",
    "# Combine the sentences from the two novels into one data frame.\n",
    "sentences1 = pd.DataFrame(alice_sents + persuasion_sents)\n",
    "sentences1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utility function to create a list of the 3000 most common words.\n",
    "def bag_of_words(text):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(3000)]\n",
    "    \n",
    "\n",
    "# Creates a data frame with features for each word in our common word set.\n",
    "# Each value is the count of the times the word appears in each sentence.\n",
    "def bow_features(sentences, common_words):\n",
    "    \n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        POS = [token.pos_\n",
    "              for token in sentence\n",
    "              if (\n",
    "                  not token.is_punct\n",
    "                  and not token.is_stop\n",
    "                  and token.pos_ in common_words\n",
    "              )]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "            \n",
    "        for pos in POS:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 500 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df\n",
    "\n",
    "# Set up the bags.\n",
    "alicewords = bag_of_words(alice_doc)\n",
    "persuasionwords = bag_of_words(persuasion_doc)\n",
    "emmawords = bag_of_words(emma_doc)\n",
    "whitmanwords = bag_of_words(whitman_doc)\n",
    "\n",
    "# Combine bags to create a set of unique words.\n",
    "common_words1 = set(alicewords + persuasionwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 500\n",
      "Processing row 1000\n",
      "Processing row 1500\n",
      "Processing row 2000\n",
      "Processing row 2500\n",
      "Processing row 3000\n",
      "Processing row 3500\n",
      "Processing row 4000\n",
      "Processing row 4500\n",
      "Processing row 5000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publicity</th>\n",
       "      <th>she</th>\n",
       "      <th>sugar</th>\n",
       "      <th>acceptable</th>\n",
       "      <th>poison</th>\n",
       "      <th>pitied</th>\n",
       "      <th>hit</th>\n",
       "      <th>umbrella</th>\n",
       "      <th>service</th>\n",
       "      <th>currant</th>\n",
       "      <th>...</th>\n",
       "      <th>after</th>\n",
       "      <th>closer</th>\n",
       "      <th>symptom</th>\n",
       "      <th>foolish</th>\n",
       "      <th>heighten</th>\n",
       "      <th>procure</th>\n",
       "      <th>those</th>\n",
       "      <th>talk</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Oh, dear, !)</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(I, shall, be, late, !, ')</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3923 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  publicity she sugar acceptable poison pitied hit umbrella service currant  \\\n",
       "0         0   0     0          0      0      0   0        0       0       0   \n",
       "1         0   0     0          0      0      0   0        0       0       0   \n",
       "2         0   0     0          0      0      0   0        0       0       0   \n",
       "3         0   0     0          0      0      0   0        0       0       0   \n",
       "4         0   0     0          0      0      0   0        0       0       0   \n",
       "\n",
       "      ...     after closer symptom foolish heighten procure those talk  \\\n",
       "0     ...         0      0       0       0        0       0     0    0   \n",
       "1     ...         0      0       0       0        0       0     0    0   \n",
       "2     ...         0      0       0       0        0       0     0    0   \n",
       "3     ...         0      0       0       0        0       0     0    0   \n",
       "4     ...         0      0       0       0        0       0     0    0   \n",
       "\n",
       "                                       text_sentence text_source  \n",
       "0  (Alice, was, beginning, to, get, very, tired, ...     Carroll  \n",
       "1  (So, she, was, considering, in, her, own, mind...     Carroll  \n",
       "2  (There, was, nothing, so, VERY, remarkable, in...     Carroll  \n",
       "3                                      (Oh, dear, !)     Carroll  \n",
       "4                         (I, shall, be, late, !, ')     Carroll  \n",
       "\n",
       "[5 rows x 3923 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create our data frame with features. This can take a while to run.\n",
    "word_counts = bow_features(sentences1, common_words1)\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "Y = word_counts['text_source']\n",
    "X = np.array(word_counts.drop(['text_sentence','text_source'], 1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    Y,\n",
    "                                                    test_size=0.4,\n",
    "                                                    random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a testing function that will fit the test the data.\n",
    "def testing_func(test, models):\n",
    "    # This is a local list used to temporarily store scores\n",
    "    scores=[]\n",
    "    # This list serves the For loop below putting the variables in  \n",
    "    train = test.fit(models[0], models[1])\n",
    "\n",
    "    print('Training set score:', test.score(models[0], models[1]))\n",
    "    print('\\nTest set score:', test.score(models[2], models[3]))\n",
    "    print('\\nCV score:', np.mean(cross_val_score(test, models[0], models[1], cv=5)))                                           \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 0\n",
    "Improve performance on test above 93% or on CV above 90%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = [X_train, y_train, X_test, y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9940438871473354\n",
      "\n",
      "Test set score: 0.9050751879699248\n",
      "\n",
      "CV score: 0.9025041950040216\n",
      "\n",
      "It took the following time to complete this task: 0:08:49.350229\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "rfc = ensemble.RandomForestClassifier(n_estimators=2000, n_jobs=4)\n",
    "testing_func(rfc, models)\n",
    "print('\\nIt took the following time to complete this task:', datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEFCAYAAAAMk/uQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+VJENoNYx/cAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11e1798d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took the following time to complete this task: 0:06:50.654666\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "# Try to figure out the best Lambda for Lasso by running the test with the PCA X variable and mapping them out.\n",
    "# Temporary lists to store the results\n",
    "yl = []\n",
    "cl = []\n",
    "\n",
    "# For loop to run the model with different Lambdas represented as L\n",
    "numb = np.arange(0.01, 1, 0.01)\n",
    "for l in numb:\n",
    "    lr = LogisticRegression(C=l, penalty='l1')\n",
    "    train = lr.fit(X_train, y_train)\n",
    "    yl.append(lr.score(X_train, y_train))\n",
    "    cl.append(np.mean(cross_val_score(lr, X_train, y_train, cv=5)))\n",
    "\n",
    "lambda_number = cl.index(max(cl)) + 1\n",
    "# Plot the results of the accuracy and Cross Validation Score\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('Score')\n",
    "ax1.scatter(x=numb, y=yl, c='b', marker=\"s\", label='Score')\n",
    "ax1.scatter(x=numb, y=cl, s=10, c='r', marker=\"^\", label='Cross Validation')\n",
    "plt.legend(loc='upper left');\n",
    "plt.show()\n",
    "print('It took the following time to complete this task:', datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9893416927899686\n",
      "\n",
      "Test set score: 0.8947368421052632\n",
      "\n",
      "CV score: 0.8877770132609515\n",
      "\n",
      "It took the following time to complete this task: 0:00:04.586371\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "lr = LogisticRegression(C=lambda_number)\n",
    "testing_func(lr, models)\n",
    "print('\\nIt took the following time to complete this task:', datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are {'C': 5, 'gamma': 0.05} with a score of %0.8987460815047021\n",
      "It took the following time to complete this task: 0:16:14.405171\n"
     ]
    }
   ],
   "source": [
    "# SKlearn has a tool that will optimize SVC\n",
    "start = datetime.now()\n",
    "svc = SVC()\n",
    "\n",
    "# Set the hyperparameters\n",
    "svcparameters = {'C':[5,7,9], 'gamma': \n",
    "              [0.02,0.04,0.05]}\n",
    "\n",
    "# Run it through the tool.\n",
    "grid = GridSearchCV(svc, svcparameters).fit(X_train, y_train)\n",
    "predicted = grid.predict(X_test)\n",
    "print(\"The best parameters are {} with a score of %{}\".format(grid.best_params_, grid.best_score_))\n",
    "\n",
    "#Put those parameters in a dictionary to call.\n",
    "svcparameters = grid.best_params_\n",
    "\n",
    "print('It took the following time to complete this task:', datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9689655172413794\n",
      "\n",
      "Test set score: 0.9135338345864662\n",
      "\n",
      "CV score: 0.9037659735363197\n",
      "\n",
      "It took the following time to complete this task: 0:03:37.239193\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "svc = SVC(C=svcparameters['C'], gamma=svcparameters['gamma'])\n",
    "testing_func(svc, models)\n",
    "print('\\nIt took the following time to complete this task:', datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are {'learning_rate': 0.5, 'n_estimators': 200} with a score of %0.8818181818181818\n",
      "It took the following time to complete this task: 0:11:56.293995\n"
     ]
    }
   ],
   "source": [
    "# Run the same optimization tool for gradient boosting\n",
    "start = datetime.now()\n",
    "clf = ensemble.GradientBoostingClassifier()\n",
    "parameters = {'n_estimators':[100, 200], 'learning_rate': [ 0.5, 1.0]}\n",
    "grid = GridSearchCV(clf, parameters).fit(X_train, y_train)\n",
    "predicted = grid.predict(X_test)\n",
    "print(\"The best parameters are {} with a score of %{}\".format(grid.best_params_, grid.best_score_))\n",
    "\n",
    "#Put those parameters in a dictionary to call.\n",
    "parameters = grid.best_params_\n",
    "print('It took the following time to complete this task:', datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.977742946708464\n",
      "\n",
      "Test set score: 0.8980263157894737\n",
      "\n",
      "CV score: 0.8858980947512134\n",
      "\n",
      "It took the following time to complete this task: 0:10:28.739975\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "clf = ensemble.GradientBoostingClassifier(n_estimators=parameters['n_estimators'], \n",
    "                                          learning_rate=parameters['learning_rate'])\n",
    "testing_func(clf, models)\n",
    "print('\\nIt took the following time to complete this task:', datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 500\n",
      "Processing row 1000\n",
      "Processing row 1500\n",
      "\n",
      "It took the following time to complete this task: 0:00:04.787884\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "emma_sentences = pd.DataFrame(emma_sents)\n",
    "emma_bow = bow_features(emma_sentences, common_words1)\n",
    "\n",
    "print('\\nIt took the following time to complete this task:', datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set score: 0.6580909768829232\n",
      "\n",
      "It took the following time to complete this task: 0:00:01.487441\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "# Now we can model it!\n",
    "# Let's use logistic regression again.\n",
    "\n",
    "# Combine the Emma sentence data with the Alice data from the test set.\n",
    "X_Emma_test = np.concatenate((\n",
    "    X_train[y_train[y_train=='Carroll'].index],\n",
    "    emma_bow.drop(['text_sentence','text_source'], 1)\n",
    "), axis=0)\n",
    "y_Emma_test = pd.concat([y_train[y_train=='Carroll'],\n",
    "                         pd.Series(['Austen'] * emma_bow.shape[0])])\n",
    "\n",
    "# Model.\n",
    "print('\\nTest set score:', lr.score(X_Emma_test, y_Emma_test))\n",
    "lr_Emma_predicted = lr.predict(X_Emma_test)\n",
    "pd.crosstab(y_Emma_test, lr_Emma_predicted)\n",
    "print('\\nIt took the following time to complete this task:', datetime.now() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 1\n",
    "Find out whether your new model is good at identifying Alice in Wonderland vs any other work, Persuasion vs any other work, or Austen vs any other work. This will involve pulling a new book from the Project Gutenberg corpus (print(gutenberg.fileids()) for a list) and processing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 500\n",
      "Processing row 1000\n",
      "Processing row 1500\n",
      "\n",
      "It took the following time to complete this task: 0:00:05.399147\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "whitman_sentences = pd.DataFrame(whitman_sents)\n",
    "whitman_bow = bow_features(whitman_sentences, common_words1)\n",
    "print('\\nIt took the following time to complete this task:', datetime.now() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alice vs. Whitman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "New_Alice_Whitman = word_counts.drop(word_counts.text_source=='Austen', axis=0)\n",
    "New_Alice_Whitman = pd.concat([New_Alice_Whitman, whitman_bow], axis=0)\n",
    "y_Alice = New_Alice_Whitman['text_source']\n",
    "X_Alice = np.array(New_Alice_Whitman.drop(['text_sentence','text_source'], 1))\n",
    "\n",
    "X_Alice_train, X_Alice_test, y_Alice_train, y_Alice_test = train_test_split(X_Alice, \n",
    "                                                    y_Alice,\n",
    "                                                    test_size=0.4,\n",
    "                                                    random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set score: 0.7208303507516106\n",
      "\n",
      "It took the following time to complete this task: 0:00:26.657758\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "# Now we can model it!\n",
    "# Let's use svc again.\n",
    "# Model.\n",
    "print('\\nTest set score:', svc.score(X_Alice_test, y_Alice_test))\n",
    "\n",
    "print('\\nIt took the following time to complete this task:', datetime.now() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persuasion vs. Whitman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "New_Persuasion_Whitman = word_counts.drop(word_counts.text_source=='Austen', axis=0)\n",
    "New_Persuasion_Whitman = pd.concat([New_Persuasion_Whitman, whitman_bow], axis=0)\n",
    "y_Persuasion = New_Persuasion_Whitman['text_source']\n",
    "X_Persuasion = np.array(New_Persuasion_Whitman.drop(['text_sentence','text_source'], 1))\n",
    "\n",
    "X_Persuasion_train, X_Persuasion_test, y_Persuasion_train, y_Persuasion_test = train_test_split(X_Persuasion, \n",
    "                                                    y_Persuasion,\n",
    "                                                    test_size=0.4,\n",
    "                                                    random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set score: 0.7208303507516106\n",
      "\n",
      "It took the following time to complete this task: 0:00:25.823148\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "# Now we can model it!\n",
    "\n",
    "# Model.\n",
    "print('\\nTest set score:', svc.score(X_Persuasion_test, y_Persuasion_test))\n",
    "\n",
    "print('\\nIt took the following time to complete this task:', datetime.now() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Austen vs. Whitman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 500\n",
      "Processing row 1000\n",
      "Processing row 1500\n",
      "Processing row 2000\n",
      "Processing row 2500\n",
      "Processing row 3000\n",
      "Processing row 3500\n",
      "Processing row 4000\n",
      "Processing row 4500\n",
      "Processing row 5000\n",
      "Processing row 5500\n",
      "Processing row 6000\n",
      "Processing row 6500\n",
      "Austen     5318\n",
      "Whitman    1669\n",
      "Name: text_source, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "sentences2 = pd.DataFrame(emma_sents + persuasion_sents + whitman_sents)\n",
    "New_Austen_Whitman = bow_features(sentences2, common_words1)\n",
    "y_Austen = New_Austen_Whitman['text_source']\n",
    "X_Austen = np.array(New_Austen_Whitman.drop(['text_sentence','text_source'], 1))\n",
    "\n",
    "X_Austen_train, X_Austen_test, y_Austen_train, y_Austen_test = train_test_split(X_Austen, \n",
    "                                                    y_Austen,\n",
    "                                                    test_size=0.4,\n",
    "                                                    random_state=0)\n",
    "print(New_Austen_Whitman['text_source'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set score: 0.7373881932021467\n",
      "\n",
      "It took the following time to complete this task: 0:00:21.577526\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "# Model.\n",
    "print('\\nTest set score:', svc.score(X_Austen_test, y_Austen_test))\n",
    "\n",
    "print('\\nIt took the following time to complete this task:', datetime.now() - start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
